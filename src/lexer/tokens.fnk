{slice} = import '@fink/std-lib/str.fnk'
{match_all, rx, split} = import '@fink/std-lib/regex.fnk'
{length} = import '@fink/std-lib/iter.fnk'



get_loc = fn start, text:
  text_len = length text
  lines = split text, rx'\n'
  lines_len = length lines

  line = start.line - 1 + lines_len
  column = match lines_len:
    1:
      start.column + text_len
    else:
      [..., last] = lines
      length last

  {pos: start.pos + text_len, line, column}



fink_lex = rx'
  ^(
    (?<keyword>\b(
      fn|match|fold|unfold|else|map|filter|while|until
      |await|import|try|throw|rec|list|true|false|pipe
      |in|or|and|not
    )\b)
    |(?<ignorable>\s+)
    |(?<comment_sl>#.*?(?=\n))
    |(?<comment_ml>---[\s\S]*?---)

    |(?<str_start>\'|")

    |(?<jsx_elem_close>/>)
    |(?<jsx_frag_open><>)
    |(?<jsx_elem_start>(<(?=\w)))

    |(?<compare>((!=)|(==)|(<=)|(>=)|<))
    |(?<assign>=)

    |(?<overloaded>[>{}])

    |(?<arithm>[-+*/%^](?=\s))
    |(?<prefix_neg>-(?=\S))

    |(?<empty>\b_\b)

    |(?<number>[0-9][\._a-fA-Fxobe+0-9-]*)
    |(?<ident>[_$\p{L}][_$\p{L}\p{N}]*)

    |(?<spread>\.\.\.)
    |(?<member>\.)
    |(?<operator>[?|])
    |(?<grouping>[()[\]])
    |(?<terminator>[,:])
    |(?<error>.)
    |(?<end>$)
  )'


str_sq_lex = rx"
  ^(
    (?<str_end>')
    |(?<str_expr_start>\$\{)
    |(?<str_text>(?:[^'\\]|\\.)+?(?='|$|(\$\{)))
    |(?<end>$)
  )
"

str_dq_lex = rx'
  ^(
    (?<str_end>")
    |(?<str_expr_start>\$\{)
    |(?<str_text>(?:[^"\\]|\\.)+?(?="|$|(\$\{)))
    |(?<end>$)
  )
'


jsx_lex = rx'
  ^(
    (?<jsx_frag_open><>)
    |(?<jsx_frag_close></>)
    |(?<jsx_elem_start>(<(?=\w)))
    |(?<jsx_elem_close>(</.+?>))
    |(?<jsx_expr_start>\{)
    |(?<jsx_text>[^<{]+)
    |(?<end>$)
  )
  '

jsx_elem_lex = rx'
  ^(
    (?<ignorable>\s+)
    |(?<comment_sl>#.*?(?=\n))
    |(?<comment_ml>---[\s\S]*?---)

    |(?<str_start>\'|")

    |(?<jsx_elem_close>/>)
    |(?<jsx_frag_open><>)
    |(?<jsx_elem_start>(<(?=\w)))

    |(?<compare>((!=)|(==)|(<=)|(>=)|<))
    |(?<assign>=)

    |(?<overloaded>[>{}])

    |(?<arithm>[-+*/%^](?=\s))
    |(?<prefix_neg>-(?=\S))

    |(?<empty>\b_\b)

    |(?<number>[0-9][\._a-fA-Fxobe+0-9-]*)
    |(?<ident>[_$\p{L}][_$\p{L}\p{N}]*)

    |(?<spread>\.\.\.)
    |(?<member>\.)
    |(?<operator>[?|])
    |(?<grouping>[()[\]])
    |(?<terminator>[,:])
    |(?<error>.)
    |(?<end>$)
  )'



get_token_type = fn matched, value, matchers:

  match matched.groups:
    {str_start: {}}: 'str-start'
    {str_text: {}}: 'str-text'
    {str_end: {}}: 'str-end'
    {str_expr_start: {}}: 'str-expr-start'

    {comment_sl: {}}: 'comment-sl'
    {comment_ml: {}}: 'comment-ml'

    {ignorable: {}}: 'ignorable'
    {prefix_neg: {}}: 'prefix_neg'
    {ident: {}}: 'ident'
    {number: {}}: 'number'

    {error: {}}: 'error'
    {end: {}}: 'end'

    {jsx_elem_start: {}}: 'jsx-elem-start'
    {jsx_elem_close: {}}: 'jsx-elem-close'
    {jsx_frag_open: {}}: 'jsx-frag-open'
    {jsx_frag_close: {}}: 'jsx-elem-close'
    {jsx_expr_start: {}}: 'jsx-expr-start'
    {jsx_text: {}}: 'jsx-text'

    {overloaded: {}}:
      [[curr_matcher, cc], parent] = matchers
      match value:
        '{': match curr_matcher:
          jsx_elem_lex: 'jsx-expr-start'
          else: value

        '}':
          [parent_lex] = parent
          match true:
            cc == 0 and parent_lex == str_dq_lex: 'str-expr-end'
            cc == 0 and parent_lex == str_sq_lex: 'str-expr-end'
            cc == 0 and parent_lex == jsx_lex: 'jsx-expr-end'
            cc == 0 and parent_lex == jsx_elem_lex: 'jsx-expr-end'
            else: value

        --- istanbul ignore else TODO: cov should be done by loxia ---
        '>': match curr_matcher:
          jsx_elem_lex: 'jsx-elem-end'
          else: value

    else:
      value



get_next_token = fn matchers, code, start:
  [[lex]] = matchers
  code_slice = slice code, start.pos

  [matched] = match_all code_slice, lex
  [value] = matched
  type = get_token_type matched, value, matchers


  end = get_loc start, value
  token = {type, value, loc: {start, end}}
  [token, end]




get_next_matchers = fn token, matchers:
  [curr_matcher, ...parent_matchers] = matchers

  match token:
    # {type: 'jsx-elem-close', value: '/>'}: parent_matchers
    {type: 'jsx-elem-close'}: parent_matchers

    {type: 'jsx-frag-open'}: [[jsx_lex, 0], ...matchers]
    {type: 'jsx-elem-start'}: [[jsx_elem_lex, 0], ...matchers]
    {type: 'jsx-elem-end'}: [[jsx_lex, 0], ...parent_matchers]

    {type: 'jsx-expr-start'}: [[fink_lex, 0], ...matchers]
    {type: 'jsx-expr-end'}: parent_matchers

    {type: 'str-start', value: '"'}: [[str_dq_lex, 0], ...matchers]
    {type: 'str-start', value: "'"}: [[str_sq_lex, 0], ...matchers]
    {type: 'str-end'}: parent_matchers

    {type: 'str-expr-start'}: [[fink_lex, 0], ...matchers]

    {type: 'str-expr-end'}: parent_matchers

    else: [curr_matcher, ...parent_matchers]



update_matcher_state = fn token, matchers:
  [[curr_m, curr_cc], ...rest] = matchers

  next_cc = match token:
    {value: '{'}:
      curr_cc + 1
    {value: '}'}:
      curr_cc - 1
    else:
      curr_cc

  [[curr_m, next_cc], ...rest]



tokenize = fn code, start={pos: 0, line: 1, column: 0}:
  initial_ctx = rec:
    code
    start
    matchers: [[fink_lex, 0], []]


  pipe:
    unfold  , {code, start, matchers}=initial_ctx:
      [token, next_start] = get_next_token matchers, code, start

      curr_matchers = update_matcher_state token, matchers

      next_matchers = get_next_matchers token, curr_matchers

      [token, {code, start: next_start, matchers: next_matchers}]



# TODO: should prattler allow registering for token-type + value
# if so, we could simply test on type being 'keyword'
ident_or_keyword = list:
  'ident', 'import', 'fn', 'pipe', 'match', 'else'
  'fold', 'unfold', 'map', 'filter', 'while', 'until'
  'list', 'rec', 'await', 'try', 'throw',
  'and', 'or', 'not', 'in', 'true', 'false'



